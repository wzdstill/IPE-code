\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{amsfonts,amssymb,amsmath}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\pp}{\mathsf{pp}}
\newcommand{\mk}{\mathsf{mk}}
\newcommand{\m}{\textbf{M}}
\newcommand{\G}{\textbf{G}}
\newcommand{\negl}{\textbf{negl}}
\newcommand{\Setup}{\textbf{Setup}}
\newcommand{\Keygen}{\textbf{Keygen}}
\newcommand{\Enc}{\textbf{Enc}}
\newcommand{\Dec}{\textbf{Dec}}



\begin{document}
\title{Inner Product Encryption with Compact Ciphertexts over Lattices}
\author{}
\date{\today}
\maketitle

\begin{abstract}

\end{abstract}

\input{intro}

\input{prelim}
\section{Preliminaries}
\textbf{Notation.} Let $\lambda$ be the security parameter, and let PPT denote probabilistic polynomial time. For any integer $q \ge 2$, we let $\Z_{q}$ denote the ring of integers modulo $q$ and we represent $\Z_{q}$ as integers in $(-q/2,q/2]$. We use bold uppercase letters to denote matrices \textbf{M}, and bold lowercase letters to denote vectors \textbf{\emph{v}}. We write  $\widetilde{\m}$ to denote the Gram-Schmidt orthogonalization of \m. We write $[n]$ to denote the set $\{1,...n\}$, and $|\textbf{t}|$ to denote the number of bits in the string \textbf{t}. We denote the $i$-bit of \textbf{t} by \textbf{t}[i]. We say a function \negl(.): $\mathbb{N} \to (0,1)$ is negligible, if for every constant $c \in \mathbb{N} $, \negl(.)$ < n^{-c}$ for sufficiently large $n$.

\subsection{Predicate Encryption}
We review the syntax of predicate encryption.\\[0.2cm]
\textbf{Definition}.  A (key-police) predicate encryption scheme for the class of predicates $\mathcal{F}$ over attributes $\Sigma$ consists of four PPT algorithms \textbf{Setup,KeyGen,Enc,\\Dec} such that :
\begin{itemize}
\item \textbf{Setup} takes as input a security parameter $n$ and output a set of public parameters $\pp$ and a master secret key $\mk$.
\item \textbf{KeyGen} takes as input the master secret key $\mk$ and a (description of a) predicate $ f \in \mathcal{F}$. It output a key $sk_{f}$.
\item \textbf{Enc} takes as input the public parameters $\pp$, an attribute $I \in \Sigma$ , and a message $M$ in some associated message space $\mathcal{M}$. It returns a ciphertext $C$.
\item \textbf{Dec} takes as input a secret key $sk_{f}$ and a ciphertext $C$. It output either a message $M$ or the distinguished symbol $\perp$.
\end{itemize}
For correctness, we require that for all $n$ ,all ($\pp$,$\mk$) generated by \textbf{Setup}($1^{n}$), all $ f \in \mathcal{F}$, any key $sk_{f}\leftarrow$ \textbf{KeyGen}(\textbf{sk},$f$), all  $I \in \Sigma$, and any ciphertext $C \leftarrow $ \textbf{Enc}($\pp$,$I,M$):
\begin{itemize}
\item If $f(I)=1$, then \textbf{Dec}($\textbf{sk}_{f}$,C)$=M$.
\item If $f(I)=0$, then \textbf{Dec}($\textbf{sk}_{f}$,C)$=\perp$ with all but negligible probability.
\end{itemize}
We next review the security definition of predicate encryption. Our definition of security is "selective", in the sense that the adversary must commit to its challenger attributes before seeing any secret keys.\\[0.2cm]
\textbf{Definition}. A predicate encryption scheme with respect to $\mathcal{F}$ and $\Sigma$ is $attribute\\hiding$ if for all probabilistic polynomial-time adversaries $\mathcal{A}$ in the following experiment is negligible in the security parameter $n$:
\begin{enumerate}
\item $\mathcal{A}$($1^{n}$) outputs $I_{0},I_{1} \in \Sigma$.\\
\item \textbf{Setup}($1^{n}$) is run to generate $\pp$ and $\mk$, and the adversary is given $\pp$.\\
\item $\mathcal{A}$ may adaptively request keys for any predicates $f_{1},...,f_{l} \in \mathcal{F}$ subject to the restriction that $f_{i}(I_{0})=f_{i}(I_{1})$ for all $i$. In response, $\mathcal{A}$ is given the corresponding keys $\textbf{sk}_{f_{i}}\leftarrow \textbf{KeyGen}(\mk,f_{i})$.\\
\item $\mathcal{A}$ outputs two equal-length messages $M_{0},M_{1}$. If there is an $i$ for which $f_{i}(I_{0})=f_{i}(I_{1})=1$, then it is required that $M_{0}=M_{1}$. A random bit $b$ is chosen, and $\mathcal{A}$ is given the ciphertext $C\leftarrow \textbf{Enc}(\pp,I_{b},M_{b})$.\\
\item The adversary may continue to request keys for additional predicates, subject to the same restrictions as before.\\
\item $\mathcal{A}$ outputs a bit $b^{'}$, and succeeds if $b=b^{'}$. The advantage of $\mathcal{A}$ is the absolute value of the difference between its success probability and 1/2.
\end{enumerate}
We say the scheme is weakly $attribute\ hiding$ if the same condition holds for adversaries $\mathcal{A}$ that are only allowed to request keys for predicates $f_{i}$ with $f_{i}(I_{0})=f_{i}(I_{1})=0$. We say the scheme is $payload\ hiding$ if we require $I_{0}=I_{1}$.\\
\subsection{lattice}
A (full-rank) lattice in $\mathbb{R}^{n}$ is $\Lambda=\{\sum^{n}_{i=1}x_{i}\textbf{b}_{i}:x_{i} \in \mathbb{Z}\}$, where $\textbf{b}_{1},..,\textbf{b}_{n} \in \mathbb{R}^{n}$ are linearly independent over $\mathbb{R}^{n}$. Matrix $\textbf{B}=[\textbf{b}_{1},...,\textbf{b}_{n}]$ is a basis of lattice $\Lambda$. For $\textbf{A} \in \mathbb{Z}^{n \times m}_{q}$ and $\textbf{u} \in \mathbb{Z}^{n}_{q}$, we define lattices and their shift:

~~~~~~~~~~~~$\Lambda_{q}(\textbf{A})=\{\textbf{y} \in \mathbb{Z}^{m}: \exists \textbf{s} \in \mathbb{Z}^{n}_{q}\ such\ that\  \textbf{y}\equiv \textbf{A}^{T}\textbf{s} (mod\ q)\}$£¬\\

~~~~~~~~~~~~$\Lambda^{\perp}_{q}(\textbf{A})=\{\textbf{e} \in \mathbb{Z}^{m}: \textbf{Ae} \equiv \textbf{0} (mod\ q)\}$, \\

~~~~~~~~~~~~$\Lambda^{\textbf{u}}_{q}(\textbf{A})=\{\textbf{e} \in \mathbb{Z}^{m}: \textbf{Ae} \equiv \textbf{\textbf{u}} (mod\ q)\}$.\\

Let $\Lambda$ be a discrete subset of $\mathbb{Z}^{m}$. For any vector $\textbf{c} \in \mathbb{R}^{n}$, and any positive parameter $\sigma \in \mathbb{R}$, let $\rho_{\sigma,\textbf{c}}(\textbf{x})=exp(-\pi ||\textbf{x}-\textbf{c}||^{2}/\sigma^{2})$ be the Gaussian function on $\mathbb{R}^{n}$ with center $\textbf{c}$ and parameter $\sigma$. Next, we let $\rho_{\sigma,\textbf{c}}(\Lambda)=\sum_{\textbf{x} \in \Lambda} \rho_{\sigma,\textbf{c}}(\textbf{x})$ be the discrete integral of $\rho_{\sigma,\textbf{x}}$ over $\Lambda$, and let $\mathcal{D}_{\Lambda,\sigma,\textbf{c}}(\textbf{y})=\frac{\rho_{\sigma,\textbf{c}}(\textbf{y})}{\rho_{\sigma,\textbf{c}}(\Lambda)}$. We abbreviate this as $\mathcal{D}_{\Lambda,\sigma}$ when $\textbf{c}=0$.\

Let $S^{m}$ denote the set of vectors in $\mathbb{R}^{m+1}$ whose length is 1. Then the norm of a matrix $\textbf{R} \in \mathbb{R}^{m\times m}$ is defined to be $\textbf{sup}_{\textbf{x} \in S^{m}}||\textbf{Rx}||$. Then we have the following lemma. which bounds the norm for some specified distributions.\\[0.2cm]
\textbf{Lemma}.Regarding the norm defined above, we have the following bounds:
\begin{itemize}
\item Let $\textbf{R}\in \{-1,1\}^{m\times m}$ be chosen at random, then we have \textbf{Pr}$[||\textbf{R}||> 12\sqrt{2m}]< e^{-2m}$.
\item Let $\textbf{R}$ be sampled from $\mathbb{D}_{\mathbb{Z}^{m\times m},\sigma}$, then we have \textbf{Pr}$[||\textbf{R}||> \sigma\sqrt{m}]< e^{-2m}$.
\end{itemize}

Micciancio and Peikert introduced a $universal$ structured (primitive) matrix $\textbf{G}$, typically in $\mathbb{Z}^{n\times \Omega(n \log q)}_{q}$, also known as the "gadget matrix", more precisely, let $\textbf{g}=(1,2,...2^{k-1}) \in \mathbb{Z}^{k}_{q}$ for $k=\lceil \log_{2}q \rceil$, then $\textbf{G}=\textbf{g}^{T}\otimes \textbf{I}_{n} \in \mathbb{Z}^{n\times nk}_{q}$.\\[0.2cm]
\textbf{Theorem}.Let $q\geq 2, n\geq 1, k=\lceil \log_{2}q \rceil$, and $\bar{m}=nk$ be integers. Let $\textbf{G}$ be as above. Then the lattice $\Lambda^{\perp}_{q}(\textbf{G})$ has a known basis $\textbf{S} \in \mathbb{Z}^{\bar{m}\times \bar{m}}$ with $||\tilde{\textbf{S}}||\leq \sqrt{5}$ and $||\textbf{S}||\leq$ max$ \{\sqrt{5},\sqrt{k}\}$.\\[0.2cm]
\textbf{Definition}. Let $\textbf{A}\in \mathbb{Z}^{n\times m}_{q}$ and $\textbf{G}\in \mathbb{Z}^{n\times w}_{q}$ be matrices with $m\geq w\geq n$. We say a matrix $\textbf{R}\in \mathbb{Z}^{(m-w)\times w}_{q}$ is a $G$-trapdoor with tag $\textbf{H}\in GL_{n}(\mathbb{Z}_{q})\subseteq \mathbb{Z}^{n\times n}_{q}$ if $\textbf{A}[\textbf{R};\textbf{I}_{w}]=\textbf{HG}$. The quality of the trapdoor is measured by $s_{1}(\textbf{R})$.\\[0.2cm]
\textbf{Theorem}. We set $k=\lceil \log q\rceil$ and $m=\bar{m}+nk$ for simplicity of notation.
\begin{itemize}
\item \textbf{GenTrap}($\bar{\textbf{A}},\textbf{H}$): Given a matrix $\bar{\textbf{A}}\in \mathbb{Z}^{n\times \bar{m}}_{q}$, an invertible matrix $\textbf{H}\in GL_{n}(\mathbb{Z}_{q})$, and a distribution $D$ over $\mathbb{Z}_{q}$, it outputs $\textbf{A}=[\bar{\textbf{A}}|\textbf{HG}-\bar{\textbf{A}}\textbf{R}]\in \mathbb{Z}
^{n\times (\bar{m}+nk)}_{q}$ and its trapdoor $\textbf{R}\in \mathbb{Z}
^{\bar{m}\times nk}_{q}$ with tag $\textbf{H}$, where $\textbf{R}$ is chosen from distribution $D$.\\
In particular, we often set  $q$ as an odd prime, $\bar{m}=n\log q+\omega(\log k), D=U(\{-1,+1\})$, and choose $\bar{\textbf{A}}$ from $\mathbb{Z}^{n\times \bar{m}}_{q}$ uniformly at random. These settings yield the obtained matrix $\textbf{A}$ as $\textbf{negl}(\varepsilon)$-uniform and $s_{1}(\textbf{R})\leq C(\sqrt{\bar{m}}+\sqrt{nk})$ with overwhelming probability.
\item \textbf{SampleD}($\textbf{R,A,H,u},s$): The input is $\textbf{A}\in \mathbb{Z}^{n\times m}_{q}$, its trapdoor $\textbf{R}\in \mathbb{Z}^{\bar{m}\times nk}_{q}$ with tag $\textbf{H}\in GL_{n}(\mathbb{Z}_{q})$, and a target vector $\textbf{u}$, and Gaussian parameter $s> \sqrt{s_{1}(\textbf{R})^{2}+1}.\sqrt{7}.\omega(\sqrt{\lg n})$. It outputs $\textbf{x}$ according to a distribution statistically close to $D_{\Lambda^{\textbf{u}}_{q}(\textbf{A}),s}$; roughly speaking, it samples $\textbf{x}$ from $D^{m}_{\mathbb{Z},s}$ conditioned on $\textbf{Ax}=\textbf{u}$.
\end{itemize}
We recall an adapted version for lattice of the $generalized$ leftover hash lemma.\\[0.2cm]
\textbf{Lemma}. Let $q$ be an odd prime. Suppose that $m>(n+t)\log q + \omega(\log n)$. Let $\textbf{R}\in \{-1,1\}^{m\times k}$ be chosen uniformly at random for some polynomial $k=k(n)$. Let $\textbf{A,B}$ be matrix chosen randomly from $\mathbb{Z}^{n\times m}_{q}$, $\mathbb{Z}^{n\times k}_{q}$ respectively. Then, for all vectors $\textbf{w}\in \mathbb{Z}^{m}$, the two following distribution are statistically close:\

~~~~~~~~~~~~~~~~~~~~~~~~~~~~$(\textbf{A,AR},\textbf{R}^{T}\textbf{w})\approx (\textbf{A,B},\textbf{R}^{T}\textbf{w})$



\subsection{Learning with Errors}
The LWE problem was introduced by Regev. For vector $s\in \mathbb{Z}^{n}_{q}$ and distribution $\chi$ over $\mathbb{Z}_{q}$, let $A(\textbf{s},\chi)$ be distribution over $\mathbb{Z}^{n}_{q}\times \mathbb{Z}_{q}$ defined by taking samples $\textbf{a}\leftarrow \mathbb{Z}^{n}_{q}$ and $x\leftarrow \chi$, and outputting $(\textbf{a},\textbf{a}^{T}\textbf{s}+x)$.\\

\textbf{Definition(The LWE Problem and assumption)}. For integer $q=q(n)$ and distribution $\chi$ over $\mathbb{Z}_{q}$, the learning with errors problem(decision) is to distinguish distribution $A(\textbf{s},\chi)$ and $U(\mathbb{Z}^{n}_{q}\times \mathbb{Z}_{q})$ for uniformly random $\textbf{s}\in \mathbb{Z}^{q}_{n}$. We say the LWE assumption holds if the two distributions are statistically close.\\

It is well-known that under (quantum) reductions ,solving the LWE problem on average case is as hard as the worst case of the approximation version of the shortest independent vector problem GapSVP$_{\gamma}$ and SIVP$_{\gamma}$, where the $\gamma$ is the approximation factor for these lattice problems. In particular, we have a reduction with parameter $\chi=\bar{\Psi}_{\alpha},\alpha q\geq 2\sqrt{n}$, and $\gamma=\bar{O}(n/\alpha)$. See[Regev05,Pei09] for more details.

\begin{definition}[The LWE Problem and assumption]
For integer $q=q(n)$ and distribution $\chi$ over $\mathbb{Z}_{q}$, the learning with errors problem(decision) is to distinguish distribution $A(\textbf{s},\chi)$ and $U(\mathbb{Z}^{n}_{q}\times \mathbb{Z}_{q})$ for uniformly random $\textbf{s}\in \mathbb{Z}^{q}_{n}$. We say the LWE assumption holds if the two distributions are statistically close.
\end{definition}


\input{Gadget}
\section{Gadget Matrices and Gadget-Based Matrix Operations}
In this section, we will show that the matrix \textbf{G} defined above can be used to support "invariant-preserving" pseudo-commutative matrix multiplication operations. We can also generalize matrix \textbf{G} and its trapdoor to other integer powers or mixed-integer products. lastly, we propose a new arrangement of matrix operations for the generalized gadget matrices that is critical to our main result.
\subsection{Matrix Operations with G via the Flattening Function $\textbf{G}^{-1}$}
Over general lattices, the only matrices that commute with \textbf{G}$\in \Z^{n \times m}_{q}$ are scaled identity matrices $\alpha\mathbf{I}$, in the sense that $\mathbf{G}.(\alpha\mathbf{I}_{m})=(\alpha\mathbf{I}_{n}).\mathbf{G}$. Note that if the \textbf{G} is padded, then $\alpha\mathbf{I}_{m}$ could alternatively be the block matrix \textbf{A} containing $\alpha\mathbf{I}_{n}$ in the appropriate with zeroes everywhere else.\

The works of Xagawa[.....] and Alperin-Sheriff and Peikert[......], among others, describe a technique that resolves this non-commutatively problem for \textbf{G}. In particular, there is an efficiently computable function $\textbf{G}^{-1}$, so that for any matrix $\textbf{B} \in \Z^{n \times m}_{q}$, so that $\textbf{G}^{-1}(\textbf{B})=\textbf{X} \in \{-1,1\}^{m \times m}$ and $\textbf{GX}=\textbf{B}$. This allows for "pseudo-commutative" multiplication of the gadget matrix \textbf{G} by any square matrix \textbf{M} of dimension $n$, by observing that
\begin{equation}
\textbf{G}.(\textbf{G}^{-1}(\textbf{MG}))=\textbf{M.G}
\end{equation}
The matrix $\textbf{X}=\G^{-1}(\textbf{B}$ has small norm independent of \textbf{B}.
\subsection{Non-Binary Gadgets $\textbf{G}_{n,r,m}$ and Function $\textbf{G}^{-1}_{n^{'},r^{'},m^{'}}(.)$}
The matrix $\G$ and its trapdoor can be extended to other integer powers or mixed-integer products by the results of [MP......]. Then, we can give a generalized notation for gadget matrices as follows:\

For any modulus $q\geq 2$, for integer $2\leq r\leq q$, let $\textbf{g}^{T}_{r}=[1,r,r^{2},...,r^{k_{r}-1}] \in \Z^{1 \times k_{r}}_{q}$ for $k_{r}=\lceil \log_{r}q \rceil$. We let $\G_{n,r}=\textbf{g}^{T}_{r}\otimes \textbf{I}_{n} \in \Z^{n \times nk_{r}}_{q}$. The public trapdoor basis can be given analogously. Similar to the above padding argument, $\G_{n,r} \in \Z^{n \times nk_{r}}_{q}$ can be padded into a matrix $\G_{n,r,m} \in \Z^{n \times m}_{q}$ for $m\geq nk_{r}$ without increasing the norm of $\widetilde{\textbf{T}_{\G_{n,r,m}}}$ from that of $\widetilde{\textbf{T}_{\G_{n,r}}}$.\

In this paper, we do not need to use $\widetilde{\textbf{T}_{\G_{n,r,m}}}$ or $\widetilde{\textbf{T}_{\G_{n,r}}}$ at all, but keep the discussion for exposition. Under this notation, the matrix $\G$ in dimension $n$ is either $\G_{n,2} \in \Z^{n \times n\log_{2}q}_{q}$ or its padded version $\G_{n,2,m} \in \Z^{n \times m}_{q}$ depending on the setting.\

We now introduce a related function - the Batch Change-of-Base function $\G^{-1}_{n^{'},r^{'},m^{'}}(.)$ - as follows:\

For any modulus $q \geq 2$, and any integer base $2 \leq r^{'} \leq q$, let integer $k_{r^{'}}=\lceil \log_{r^{'}}q \rceil$. For any integer $N^{'} \geq 2$ and $m^{'} \geq n^{'}k_{r^{'}}$ the function $\G^{-1}_{n^{'},r^{'},m^{'}}(.)$ takes as input a matrix from $\Z^{n^{'} \times m^{'}}_{q}$, first computes a matrix in $\{0,1,...,r^{'}-1\}^{n^{'}\log_{r^{'}}q \times m^{'}}$ using the $\G^{-1}$, then pads with rows of zeroes needed to form a matrix in $\{0,1,...,r^{'}-1\}^{m^{'} \times m^{'}}$. For example, the typical base-2 $\G^{-1}=\G^{-1}_{n,2,m}$ takes $\Z^{n \times m}_{q}$ to $\{0,1\}^{m \times m}$ as expected.
\subsection{Further Gadget-Based Matrix Multiplication Operations}
In this part, we propose a new arrangement of matrix operations(pseudo-commutative and non-commutative) that is critical to our main construction in this paper. First, fix any integer $n$, for some sufficiently small $l=\omega(1)\ll \log_{2}q \approx \log_{2}n$, define $l^{'}=2^{l}$ such that $l^{'}= \omega(1)< \log_{2}q \approx \log_{2}n$. Finally, fix any integer $m\geq nlk_{l^{'}}$.\

Then for any two matrices $\textbf{H} \in \Z^{n \times nl}_{q}, \textbf{X} \in \Z^{ln \times n}_{q}$, consider the following terms, in order:
\begin{enumerate}
\item First, consider the base-2, dimension-n "gadget-encoding" of $\textbf{X} \in \Z^{ln \times n}_{q}$, i.e. the matrix
\begin{equation}
\textbf{X}.\G_{n,2,m} \in \Z^{ln \times m}_{q}=\Z^{n\log_{2}(l^{'}) \times m}.
\end{equation}
\item Next, consider the base-$l^{'}$, dimension-$(nl)$ flatting(with zero-row padding) of the above:
\begin{equation}
\G^{-1}_{nl,l^{'},m}(\textbf{X}.\G_{n,2,m}) \in \{0,1,...,l^{'}-1\}^{m \times m} \subsetneq \Z^{m \times m}_{q}.
\end{equation}
\item Then, consider the base-$l^{'}$, dimension-$(nl)$ gadget-encoding of $\textbf{H} \in \Z^{n \times nl}_{q}$, i.e. the matrix
\begin{equation}
\textbf{H}.\G_{nl,l^{'},m} \in \Z^{n \times m}_{q}.
\end{equation}
\item Finally - for sufficient large $m$, we find the following relationship holds:
\begin{equation}
(\textbf{H}.\G_{nl,l^{'},m}).(\G^{-1}_{nl,l^{'},m}(\textbf{X}.\G_{n,2,m}))=(\textbf{H.X}).\G_{n,2,m} \in \Z^{n \times m}_{q}
\end{equation}
\end{enumerate}
with $\| \G^{-1}_{nl,l^{'},m}(\textbf{X}.\G_{n,2,m}) \|= \textbf{small}$, conditioned on the sufficiently small choice of $l=\omega(1)$. We emphasize that only public information, $n,m,l$, is required to perform this batch base-change-then-multiply operation, when given as input any $\textbf{M} \in \Z^{n \times m}_{q}$(equal to $\textbf{H}.\G_{nl,l^{'},m} \in \Z^{n \times m}_{q}$) and any $\textbf{X} \in \Z^{ln \times n}_{q}$.\\[0.3cm]
\textbf{Definition}. We refer to the matrix $\textbf{H}.\G_{nl,l^{'},m} \in \Z^{n \times m}_{q}$ as the predicate-encoding of $\textbf{H} \in \Z^{n \times nl}_{q}$, and to the matrix $\G^{-1}_{nl,l^{'},m}(\textbf{X}.\G_{n,2,m}) \in \{0,1,...,l^{'}-1\}^{m \times m} \subsetneq \Z^{m \times m}_{q}$ as the input-encoding of $\textbf{X} \in \Z^{ln \times n}_{q}$.



\input{scheme}
\section{scheme}

\subsection{The Construction}
Let $\lambda \in \Z^{+}$ be the security parameter and $l$ be the length of predicate and attribute vectors. Let $q$ and $m$ be positive integers. Let $\sigma$ and $\alpha$ be positive real Gaussian parameters. Define $k=\lfloor \log_{2}q\rfloor$. \\[0.2cm]
$\Setup$$(1^{\lambda},1^{l})$: On input a security parameter $\lambda$ and a parameter $l$ denoting the

~~~~length of predicate and attribute vectors, do:
\begin{enumerate}
\item Use the algorithm \textbf{GenTrap}($1^{\lambda},q,n,m$) to generate a random matrix $\textbf{A} \in \Z^{n \times m}_{q}$ together with a trapdoor $\textbf{R}_{A}$.\\

\item Choose a random matrix $\textbf{B} \in \Z^{n \times m}_{q}$.\\

\item Choose a random vector $\textbf{u} \in \Z^{n}_{q}$.
\end{enumerate}
~~Output $\pp=(\textbf{A,B,u}),\mk=\textbf{R}_{A}$.\\[0.2cm]
$\Keygen$$(\pp,\mk,\vec{v})$: On input the public parameters $\pp$, the master secret key

~~~~ $\mk$, and a predicate vector $\vec{v}=(v_{1},...,v_{l}) \in \Z^{l}_{q}$, do:
\begin{enumerate}
\item Define the input-encoding matrices
\begin{equation}
\textbf{V}^{'}= \begin{bmatrix}
v_{1} \textbf{I}_{n}\\
v_{2} \textbf{I}_{n}\\
\vdots\\
v_{l} \textbf{I}_{n}
\end{bmatrix} \in \Z^{ln \times n},~~~ \textbf{V}=\textbf{G}^{-1}_{nl,l^{'},m}(\textbf{V}^{'}.\textbf{G}_{n,2,m}) \in [l^{'}]^{m \times m}.
\end{equation}

\item Define the  matrix $\textbf{U}=\textbf{BV} \in \Z^{n \times m}_{q}$,$\textbf{A}_{v}=[\textbf{A}|\textbf{U}]$.\\

\item Using the master secret key $\mk=(\textbf{R}_{A},\sigma)$, compute $\vec{r}\leftarrow$ \textbf{SampleD}($\textbf{A}_{v},\textbf{R}_{A},\textbf{I,u},\sigma$). Then $\vec{r}$ is a vector in $\Z^{2m}$ satisfying $\textbf{A}_{v}.\vec{r}=\vec{u}$ (mod $q$).
\end{enumerate}
~~Output the secret key  $\textbf{sk}_{v}=\vec{r}$.\\[0.2cm]
$\Enc$$(\pp,\vec{w},m)$: On input public parameters $\pp$, an attribute vectors $\vec{w}$, and a

~~~~message $m \in \{0,1\}$, do:
\begin{enumerate}
\item Choose a uniformly random $\vec{s}\xleftarrow{\$} \Z^{n}_{q}$.\\

\item Choose a noise vector $\vec{e}_{0}\leftarrow D_{\Z^{m}_{q},\alpha}$ and a noise term $e\leftarrow D_{\Z_{q},\alpha}$ \\.

\item Compute $\vec{c}_{0}=\vec{s}^{T}\textbf{A}+\vec{e}^{T}_{0}$.

\item Define the matrix
\begin{equation}
\textbf{W}^{'}= \begin{bmatrix}
w_{1} \textbf{I}_{n}&w_{2} \textbf{I}_{n}&\ldots&w_{l} \textbf{I}_{n}
\end{bmatrix} \in \Z^{n \times ln},~~~ \textbf{W}=\textbf{W}^{'}.\textbf{G}_{nl,l^{'},m} \in \Z^{n \times m}_{q}.
\end{equation}
Pick a random matrix $\textbf{R}\xleftarrow{\$} \{-1,1\}^{m \times m}$, define error vector $\vec{e}^{T}_{1}=\vec{c}^{T}_{0}\textbf{R}$. Set
\begin{equation}
\vec{c}_{1}=\vec{s}^{T}(\textbf{B}+\textbf{W})+\vec{e}^{T}_{1},~~~c_{2}=\vec{s}^{T}\vec{u}+e+\lfloor \frac{q}{2} \rfloor m.
\end{equation}
\end{enumerate}
~~Output ciphertext  $\vec{ct}=(\vec{c}_{0},\vec{c}_{1},c_{2})$.\\[0.2cm]
$\Dec$$(\pp,\textbf{sk}_{v},\vec{ct},\vec{v})$: On input public parameters $\pp$ ,a secret key $\textbf{sk}_{v}$ for predicate

~~ vector $\vec{v}$, and a ciphertext $\vec{ct}$, do:
\begin{enumerate}
\item Compute the vector $\vec{c}_{v}=\vec{c}_{1}.\textbf{V}$.\\

\item Let $\vec{c}=[\vec{c}_{0}|\vec{c}_{v}]$.\\

\item Compute $ z\leftarrow c_{2}-\vec{c}\vec{r}$(mod $q$).
\end{enumerate}
~~Output $0$ if $|z|<q/4$ and $1$ otherwise.\\[0.2cm]

\subsection{Correctness}
We prove correctness of our scheme as follow.\\[0.2cm]
\textbf{lemma} For certain parameter choices, our scheme is correct.\\[0.2cm]
\textbf{Proof}. Recall that the ciphertext $\vec{ct}=(\vec{c}_{0},\vec{c}_{1},c_{2})$. For which $\vec{c}_{0}=\vec{s}^{T}\textbf{A}+\vec{e}^{T}_{0},  \vec{c}_{1}=\vec{s}^{T}(\textbf{B}+\textbf{W})+\vec{e}^{T}_{1}, c_{2}=\vec{s}^{T}\vec{u}+e+\lfloor \frac{q}{2} \rfloor m$, so
\begin{equation}
\vec{c}_{v}=\vec{c}_{1}.\textbf{V}=\vec{s}^{T}(\textbf{B}+\textbf{W}).\textbf{V}+\vec{e}^{T}_{1}.\textbf{V}=\vec{s}^{T}.\textbf{B}.\textbf{V}+\vec{s}^{T}.\textbf{W}.\textbf{V}+
\vec{e}^{T}_{1}.\textbf{V}.
\end{equation}
It's easy to check that if $\langle \vec{v},\vec{w} \rangle=0$, then $\textbf{W}.\textbf{V}=\textbf{W}^{'}.\textbf{V}^{'}.\textbf{G}_{n,2,m}=\textbf{0}$. Then $\vec{c}_{v}=\vec{s}^{T}.\textbf{B}.\textbf{V}+\vec{e}^{T}_{1}.\textbf{V}$, so
\begin{equation}
\begin{aligned}
\vec{c}&=[\vec{c}_{0}|\vec{c}_{v}]=[\vec{s}^{T}\textbf{A}+\vec{e}^{T}_{0}|\vec{s}^{T}.\textbf{B}.\textbf{V}+\vec{e}^{T}_{1}.\textbf{V}]=\vec{s}^{T}[\textbf{A}|\textbf{B}.\textbf{V}]+[\vec{e}^{T}_{0}|\vec{e}^{T}_{1}.\textbf{V}]\\
&=\vec{s}^{T}[\textbf{A}|\textbf{U}]+[\vec{e}^{T}_{0}|\vec{e}^{T}_{1}.\textbf{V}]=\vec{s}^{T}\textbf{A}_{v}+[\vec{e}^{T}_{0}|\vec{e}^{T}_{1}.\textbf{V}].
\end{aligned}
\end{equation}
And
\begin{equation}
\begin{aligned}
z&=c_{2}-\vec{c}\vec{r}(mod\ q)=\vec{s}^{T}\vec{u}+e+\lfloor \frac{q}{2} \rfloor m-\vec{s}^{T}\vec{u}-[\vec{e}^{T}_{0}|\vec{e}^{T}_{1}.\textbf{V}]\vec{r}\\
&=\lfloor \frac{q}{2} \rfloor m+\underbrace{e-[\vec{e}^{T}_{0}|\vec{e}^{T}_{1}.\textbf{V}]\vec{r}}_{\hat{\textbf{e}}}
\end{aligned}
\end{equation}
If the norm $\|\hat{\textbf{e}}\|< q/4$, then $|z|< q/4$ if and only if $m=0$, or else $m=1$. In this condition, our scheme is correct.
\subsection{security}
In this part, we show the security proof of our IPE scheme as follows:\\[0.2cm]
\textbf{Theorem} Assuming the hardness of the standard LWE assumption, Our IPE scheme described above is weakly attribute hiding.\\[0.2cm]
\textbf{Proof}: To prove the theorem we define a series games against adversary $\mathcal{A}$ that play the weak attribute hiding game. The adversary $\A$ outputs two attribute vectors $\vec{w}_{0}$ and $\vec{w}_{1}$ at the beginning of each game, and at some point output two messages $m_{0},m_{1}$. The first and last games correspond to real security game with challenge ciphertexts $\Enc(\pp,\vec{w}_{0},m_{0})$ and $\Enc(\pp,\vec{w}_{1},m_{1})$ respectively. In the intermediate games we use the "alternative" simulation algorithms $\textbf{Sim}.\Setup,\textbf{Sim}.\Keygen$, $\textbf{Sim}.\Enc$. During the course of the game the adversary can only request keys for predicate vector $\vec{v}$ such that $\langle \vec{v}, \vec{w}_{0} \rangle \neq 0$ and $\langle \vec{v}, \vec{w}_{1} \rangle \neq 0$.\\[0.2cm]
\textbf{Game0}: The challenger runs $\Setup$, answers $\A$'s secret key queries using $\Keygen$, and generates the challenge ciphertext using $\Enc$ with attribute $\vec{w}_{0}$ and message $m_{0}$.\\[0.2cm]
\textbf{Game1}: The challenger runs $\textbf{Sim}.\Setup$ with $\vec{w}^{*}=\vec{w}_{0}$, and answers $\A$'s secret key queries using $\textbf{Sim}.\Keygen$. The challenger generates the challenge ciphertext using $\textbf{Sim}.\Enc$ with attribute  $\vec{w}_{0}$ and message $m_{0}$.\\[0.2cm]
\textbf{Game2}: The challenger runs $\textbf{Sim}.\Setup$ with $\vec{w}^{*}=\vec{w}_{0}$, and answers $\A$'s secret key queries using $\textbf{Sim}.\Keygen$. The challenger generates the challenge ciphertext by choosing a uniformly random element of the ciphertext space.\\[0.2cm]
\textbf{Game3}: The challenger runs $\textbf{Sim}.\Setup$ with $\vec{w}^{*}=\vec{w}_{1}$, and answers $\A$'s secret key queries using $\textbf{Sim}.\Keygen$. The challenger generates the challenge ciphertext by choosing a uniformly random element of the ciphertext space.\\[0.2cm]
\textbf{Game4}: The challenger runs $\textbf{Sim}.\Setup$ with $\vec{w}^{*}=\vec{w}_{1}$, and answers $\A$'s secret key queries using $\textbf{Sim}.\Keygen$. The challenger generates the challenge ciphertext using $\textbf{Sim}.\Enc$ with attribute  $\vec{w}_{1}$ and message $m_{1}$.\\[0.2cm]
\textbf{Game5}: The challenger runs $\Setup$, answers $\A$'s secret key queries using $\Keygen$, and generates the challenge ciphertext using $\Enc$ with attribute $\vec{w}_{1}$ and message $m_{1}$.\\[0.2cm]
Now we define the "alternative" simulation algorithm as follows:\\[0.2cm]
\textbf{Sim.Setup}$(1^{\lambda},1^{l},\vec{w}^{*})$: On input a security parameter $\lambda$, a parameter $l$ denoting the length of predicate(and the attribute) vector, and an attribute vector $\vec{w}^{*} \in \Z^{l}_{q}$, do the following:
\begin{enumerate}
\item Choose a random matrix $\textbf{A} \xleftarrow {\$} \Z^{n \times m}_{q}$ and a random vector $\vec{u} \xleftarrow {\$} \Z^{n}_{q}$.\\

\item Choosing a random matrix $\textbf{R}^{*} \xleftarrow {\$} \{-1,1\}^{m \times m}$.\\

\item Define the matrix
\begin{equation}
\textbf{W}^{'}= \begin{bmatrix}
w^{*}_{1} \textbf{I}_{n}&w^{*}_{2} \textbf{I}_{n}&\ldots&w^{*}_{l} \textbf{I}_{n}
\end{bmatrix} \in \Z^{n \times ln},
\end{equation}
and set $\textbf{B}=\textbf{A}\textbf{R}^{*}-\textbf{W}^{'}.\textbf{G}_{nl,l^{'},m} \in \Z^{n \times m}_{q}$.
\end{enumerate}
~~Output $\pp=(\textbf{A,B,u}),\mk=(\textbf{R}^{*}, \pp, T_{G_{n,2,m}})$.\\[0.2cm]
\textbf{Sim.Keygen}$(\pp, \mk, \vec{v})$: On input a master key $\mk$ and a vector $\vec{v} \in \Z^{l}_{q}$, do the following:
\begin{enumerate}
\item Check $\langle \vec{v}, \vec{w}^{*} \rangle =0$, if so ,output $\perp$.\\

\item Define the matrix
\begin{equation}
\textbf{V}^{'}= \begin{bmatrix}
v_{1} \textbf{I}_{n}\\
v_{2} \textbf{I}_{n}\\
\vdots\\
v_{l} \textbf{I}_{n}
\end{bmatrix} \in \Z^{ln \times n},~~ \textbf{V}=\textbf{G}^{-1}_{nl,l^{'},m}(\textbf{V}^{'}.G_{n,2,m}).
\end{equation}
Set $\textbf{U}=\textbf{BV}, \textbf{A}_{v}=[\textbf{A}|\textbf{U}]=[\textbf{A}|\textbf{A}\textbf{R}^{*}\textbf{V}-\textbf{W}^{'}\textbf{V}^{'}\textbf{G}_{n,2,m}]$.\\

\item Let $\vec{r}\leftarrow$ \textbf{SampleD}$(\textbf{A},\textbf{AR}^{*}\textbf{V},\textbf{W}^{'}\textbf{V}^{'},\textbf{G}_{n,2,m},\textbf{T}_{G_{n,2,m}})$.
\end{enumerate}
~~Output the secret key $\textbf{sk}_{v}=\vec{r}$.\\[0.2cm]
\textbf{Sim.Enc}$(\pp,\vec{w},m,\mk)$: This algorithm is the same as the $\Enc$ algorithm, except that $\textbf{R}^{*}$ is used instead of the random matrix $\textbf{R}$ in step4.\\[0.3cm]
We show that each pair of games(\textbf{Game} $i$, \textbf{Game} $i+1$) are either statistically indistinguishable or computationally indistinguishable under the DLWE assumption.\\[0.2cm]
\textbf{Lemma} The view of the adversary $\A$ in \textbf{Game0} is statistically closed to the view of $\A$ in \textbf{Game1}, similarly, \textbf{Game4} is statistically closed to \textbf{Game5}.\\[0.2cm]
\textbf{Proof}. We just prove the case for \textbf{Game0} and \textbf{Game1}.\

First, The matrix $\textbf{A}$ in $\pp$ is chosen by running \textbf{Gentrap} in \textbf{Game0}, whereas it is a uniformly random matrix in $\Z^{n \times m}_{q}$ in \textbf{Game1}. Since $m$...................., by Theorem...................,the matrix \textbf{A} output by \textbf{Gentrap} is statistically indistinguishable from a uniformly random matrix, and thus the distribution of \textbf{A} in \textbf{Game0} and \textbf{Game1} are statistically close.\

Next, We show that the distribution of \textbf{B} in $\pp$ and $\vec{c}_{1}$ in ciphertext in \textbf{Game0} and \textbf{Game1} are statistically indistinguishable. The difference between $(\textbf{B},\vec{c}_{1})$ in two games is as follows:
\begin{itemize}
\item In \textbf{Game0} the matrix \textbf{B} is uniformly random in $\Z^{n \times m}_{q}$. In \textbf{Game1}, $\textbf{B}=\textbf{A}\textbf{R}^{*}-\textbf{W}^{'}\textbf{G}_{nl,l^{'},m}$. The matrix $\textbf{R}^{*}$ is uniformly chosen from $\{-1,1\}^{m \times m}$.
\item In \textbf{Game0} the challenge ciphertext components $\vec{c}_{1}$ are computed as $\vec{c}_{1}=\vec{s}^{T}(\textbf{B}+\textbf{W})+\vec{e}^{T}_{1}=\vec{s}^{T}(\textbf{B}+\textbf{W}^{'}\textbf{G}_{nl,l^{'},m})+\vec{e}^{T}_{0}\textbf{R}^{*}$, where \textbf{B} is uniformly random in $\Z^{n \times m}_{q}$ and the matrix $\textbf{R}^{*}$ is uniformly chosen from $\{-1,1\}^{m \times m}$.\\
    In \textbf{Game1} the challenge ciphertext components $\vec{c}_{1}$ is $\vec{c}_{1}=\vec{s}^{T}(\textbf{A}\textbf{R}^{*}-\textbf{W}^{'}\textbf{G}_{nl,l^{'},m}+\textbf{W}^{'}\textbf{G}_{nl,l^{'},m})+\vec{e}^{T}_{0}\textbf{R}^{*}=\vec{s}^{T}\textbf{A}\textbf{R}^{*}+\vec{e}^{T}_{0}\textbf{R}^{*}
    =(\vec{s}^{T}\textbf{A}+\vec{e}^{T}_{0})\textbf{R}^{*}$. Where $\textbf{R}^{*}$ are the same matrix used to compute the public parameters $\textbf{B}$. So the main difference between the two games is that the matrix $\textbf{R}^{*}$ is chosen by $\Enc$ and used only in the ciphertext $\vec{c}_{1}$ in \textbf{Game0}, whereas in \textbf{Game1}, it plays a double role: it's used to construct the matrix $\textbf{B}$ in $\pp$ as well as the ciphertext $\vec{c}_{1}$.
\end{itemize}

Now we show that the distributions $(\textbf{A,B},\vec{c}_{1})$ in \textbf{Game0} and \textbf{Game1} are statistically indistinguishable. for $\textbf{B}$ is uniformly random and $\textbf{R}^{*}$ is uniformly random in $\{-1,1\}$, so by the $generalized$ leftover hash lemma, the following two distributions are statistically indistinguishable:
\begin{equation}
(\textbf{A},\textbf{A}\textbf{R}^{*}-\textbf{W}^{'}\textbf{G}_{nl.l^{'},m}, \vec{e}^{T}_{0}\textbf{R}^{*})\approx_{s}(\textbf{A,B},\vec{e}^{T}_{0}\textbf{R}^{*})
\end{equation}
We add the same quantity to both sides of Equation(9), we see that the following two distributions are statistically close:
\begin{equation}
\begin{split}
(\textbf{A},\textbf{A}\textbf{R}^{*}-\textbf{W}^{'}\textbf{G}_{nl.l^{'},m},& \underbrace{\vec{s}^{T}(\textbf{A}\textbf{R}^{*}-\textbf{W}^{'}\textbf{G}_{nl.l^{'},m}+\textbf{W}^{'}\textbf{G}_{nl.l^{'},m})}
_{added\ term}+\vec{e}^{T}_{0}\textbf{R}^{*})\\
&\approx_{s}(\textbf{A,B},\underbrace{\vec{s}^{T}(\textbf{B}+\textbf{W}^{'}\textbf{G}_{nl.l^{'},m})}_{added\ term}+\vec{e}^{T}_{0}\textbf{R}^{*})\\
\end{split}
\end{equation}
The distribution on the left hand side of (10) is the distribution of $(\textbf{A,B},\vec{c}_{1})$ in \textbf{Game1}, while the right hand side is the distribution in \textbf{Game0}. So the two distributions are statistically indistinguishable.\

Next,we show the secret keys output by $\textbf{Sim}\Keygen$ are statistically indistinguishable from those output by $\Keygen$. Assuming $\sigma$ is sufficiently large, it follows from the properties of the algorithm \textbf{SampleD},  in both games the secret key is chosen from $\mathcal{D}_{\Lambda^{\vec{u}}_{q}(\textbf{A}_{v}),\sigma}$ with overwhelming probability, so the two keys are statistically indistinguishable.\\[0.3cm]
\textbf{Lemma} If the DLWE assumption holds, then the view of $\A$ in \textbf{Game1} is computationally indistinguishable from the view of $\A$ in \textbf{Game2}, similarly, \textbf{Game3} is computationally indistinguishable from \textbf{Game4}.\\[0.2cm]
\textbf{Proof}. We just prove the case for \textbf{Game1} and \textbf{Game2}.\

Suppose we are given $m+1$ LWE instances $(\vec{a}_{i},b_{i})$ for $i=0,...,m$, where either $b_{i}=\vec{s}^{T}\vec{a}_{i}+e_{i}$ for some fixed random secret $\vec{s}\xleftarrow{\$} \Z^{n}_{q}$ and discrete Gaussian noise $e_{i}\leftarrow \bar{\Psi}_{\alpha}$ or $b_{i}$ is uniformly random in $\Z_{q}$. Define the following variables:
\begin{equation}
\begin{split}
&\textbf{A}=\begin{bmatrix}
\vec{a}_{1}&\ldots&\vec{a}_{m}
\end{bmatrix} \in \Z^{n \times m}_{q} ~~~~~~~ \vec{u}=\vec{a}_{0}\\
&\vec{c}_{0}=(b_{1},...,b_{m}) \in \Z^{m}_{q}~~~~~~~~~~~~c_{2}=b_{0}+\lfloor \frac{q}{2}\rfloor m\\
\end{split}
\end{equation}
We simulate the challenger as follows:
\begin{itemize}
\item $\Setup$: Run $\textbf{Sim}.\Setup$ with $\vec{w}^{*}=\vec{w}_{0}$. and let \textbf{A,u} as defined above.

\item \textbf{Secret key queries}: Run $\textbf{Sim}.\Keygen$ .

\item \textbf{Challenge ciphertext}: Let $\vec{c}_{1}=\vec{c}_{0}\textbf{R}^{*}$(using the $\textbf{R}^{*} \in \mk$). Output $(\vec{c}_{0}, \vec{c}_{1}, c_{2})$.
\end{itemize}
In the \textbf{Sim}$\Enc$ algorithm it sets:
\begin{equation}
\begin{split}
\vec{c}_{1}&=\vec{s}^{T}(\textbf{A}\textbf{R}^{*}-\textbf{W}^{'}\textbf{G}_{nl,l^{'},m}+\textbf{W}^{'}\textbf{G}_{nl,l^{'},m})+\vec{e}^{T}_{0}\textbf{R}^{*}=\vec{s}^{T}\textbf{A}\textbf{R}^{*}+\vec{e}^{T}_{0}\textbf{R}^{*}\\
&=(\vec{s}^{T}\textbf{A}+\vec{e}^{T}_{0})\textbf{R}^{*}.
\end{split}
\end{equation}
So if $b_{i}=\vec{s}^{T}\vec{a}_{i}+e_{i}$, then $\vec{c}_{1}=\vec{c}_{0}\textbf{R}^{*}$ and the simulator is the same as a \textbf{Game1} challenger. Whereas if $b_{i}$ is random in $\Z_{q}$, then simulated ciphertext is $(\vec{c}_{0}, \vec{c}_{0}\textbf{R}^{*}, c_{2})$. By the leftover hash lemma, $\vec{c}_{0}\textbf{R}^{*}$ is uniformly random. Thus the ciphertext in this case is uniformly random and the simulator is identical to \textbf{Game2}'s challenger.\

We conclude that any efficient adversary that can distinguish \textbf{Game1} from \textbf{Game2} can solve the DLWE problem.\\[0.3cm]
\textbf{Lemma}. The view of $\A$ in \textbf{Game2} is statistically indistinguishable from the view of $\A$ in \textbf{Game3}.\\[0.2cm]
\textbf{Proof}. Note that the only difference between the two games is that the attribute vector $\vec{w}^{*}$ is equal to $\vec{w}_{0}$ in \textbf{Game2}, whereas $\vec{w}^{*}=\vec{w}_{0}$ in \textbf{Game3}. The attribute vector $\vec{w}^{*}$ appears in the public parameter $\textbf{B}=\textbf{A}\textbf{R}^{*}-\textbf{W}^{'}\textbf{G}_{nl,l^{'},m}$, by the $generalized$ leftover hash lemma $(\textbf{A,AR}^{*})$ is statistically indistinguishable from (\textbf{A},$\textbf{U}_{uni}$), where $\textbf{U}_{uni}$ is uniformly random. So $\textbf{A}\textbf{R}^{*}-\textbf{X}$ for any fixed $\textbf{X}$ is also uniformly random. It follows that the distribution of $\textbf{B}$ in the two games are statistically close.\\[0.3cm]
Now we conclude the proof of the Theorem. Suppose that there is an efficient adversary $\A$ that can win the security game. Let $\A^{(i)}$ denote the output of $\A$ in \textbf{Game} $i$. We have
\begin{equation}
|Pr[\A^{(0)}=1]-Pr[\A^{(5)}=1]|\geq\frac{1}{poly(n)}.
\end{equation}
By a standard hybrid argument, this implies that
\begin{equation}
|Pr[\A^{(i)}=1]-Pr[\A^{(i+1)}=1]|\geq\frac{1}{poly(n)}.
\end{equation}
for $i=0,...,4$. Since $\A$ is polynomial time, \textbf{Lemma(...)} implies that (14) cannot hold for $i=0$ or 4, \textbf{Lemma(...)} implies that (14) cannot hold for $i=2$. So from \textbf{Lemma(...)} $\A$ can be used to solve the DLWE problem.


\subsection{Parameter Setting}





\end{document}

